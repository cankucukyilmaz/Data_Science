{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, use_batchnorm=True):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        cur_size = input_size\n",
    "\n",
    "        for size in hidden_layers:\n",
    "            layers.append(nn.Linear(cur_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(size))\n",
    "            cur_size = size\n",
    "        \n",
    "        layers.append(nn.Linear(size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"output\", axis=1)\n",
    "y = df[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(X.shape[1], [16, 32, 16], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.734 | Acc: 52.0\n",
      "Epoch 002: | Loss: 0.726 | Acc: 52.0\n",
      "Epoch 003: | Loss: 0.717 | Acc: 52.0\n",
      "Epoch 004: | Loss: 0.709 | Acc: 52.0\n",
      "Epoch 005: | Loss: 0.701 | Acc: 53.0\n",
      "Epoch 006: | Loss: 0.694 | Acc: 55.0\n",
      "Epoch 007: | Loss: 0.687 | Acc: 57.0\n",
      "Epoch 008: | Loss: 0.680 | Acc: 58.0\n",
      "Epoch 009: | Loss: 0.673 | Acc: 60.0\n",
      "Epoch 010: | Loss: 0.667 | Acc: 61.0\n",
      "Epoch 011: | Loss: 0.661 | Acc: 63.0\n",
      "Epoch 012: | Loss: 0.655 | Acc: 63.0\n",
      "Epoch 013: | Loss: 0.649 | Acc: 65.0\n",
      "Epoch 014: | Loss: 0.644 | Acc: 65.0\n",
      "Epoch 015: | Loss: 0.638 | Acc: 65.0\n",
      "Epoch 016: | Loss: 0.633 | Acc: 66.0\n",
      "Epoch 017: | Loss: 0.628 | Acc: 67.0\n",
      "Epoch 018: | Loss: 0.623 | Acc: 67.0\n",
      "Epoch 019: | Loss: 0.618 | Acc: 68.0\n",
      "Epoch 020: | Loss: 0.614 | Acc: 69.0\n",
      "Epoch 021: | Loss: 0.609 | Acc: 69.0\n",
      "Epoch 022: | Loss: 0.605 | Acc: 69.0\n",
      "Epoch 023: | Loss: 0.600 | Acc: 69.0\n",
      "Epoch 024: | Loss: 0.596 | Acc: 70.0\n",
      "Epoch 025: | Loss: 0.592 | Acc: 70.0\n",
      "Epoch 026: | Loss: 0.587 | Acc: 71.0\n",
      "Epoch 027: | Loss: 0.583 | Acc: 71.0\n",
      "Epoch 028: | Loss: 0.580 | Acc: 72.0\n",
      "Epoch 029: | Loss: 0.576 | Acc: 72.0\n",
      "Epoch 030: | Loss: 0.572 | Acc: 73.0\n",
      "Epoch 031: | Loss: 0.569 | Acc: 73.0\n",
      "Epoch 032: | Loss: 0.565 | Acc: 73.0\n",
      "Epoch 033: | Loss: 0.562 | Acc: 73.0\n",
      "Epoch 034: | Loss: 0.558 | Acc: 73.0\n",
      "Epoch 035: | Loss: 0.555 | Acc: 73.0\n",
      "Epoch 036: | Loss: 0.552 | Acc: 74.0\n",
      "Epoch 037: | Loss: 0.549 | Acc: 74.0\n",
      "Epoch 038: | Loss: 0.546 | Acc: 74.0\n",
      "Epoch 039: | Loss: 0.543 | Acc: 74.0\n",
      "Epoch 040: | Loss: 0.540 | Acc: 74.0\n",
      "Epoch 041: | Loss: 0.537 | Acc: 75.0\n",
      "Epoch 042: | Loss: 0.534 | Acc: 75.0\n",
      "Epoch 043: | Loss: 0.531 | Acc: 75.0\n",
      "Epoch 044: | Loss: 0.528 | Acc: 75.0\n",
      "Epoch 045: | Loss: 0.526 | Acc: 75.0\n",
      "Epoch 046: | Loss: 0.523 | Acc: 75.0\n",
      "Epoch 047: | Loss: 0.521 | Acc: 76.0\n",
      "Epoch 048: | Loss: 0.518 | Acc: 76.0\n",
      "Epoch 049: | Loss: 0.516 | Acc: 76.0\n",
      "Epoch 050: | Loss: 0.514 | Acc: 77.0\n",
      "Epoch 051: | Loss: 0.511 | Acc: 77.0\n",
      "Epoch 052: | Loss: 0.509 | Acc: 77.0\n",
      "Epoch 053: | Loss: 0.507 | Acc: 78.0\n",
      "Epoch 054: | Loss: 0.505 | Acc: 78.0\n",
      "Epoch 055: | Loss: 0.503 | Acc: 77.0\n",
      "Epoch 056: | Loss: 0.501 | Acc: 77.0\n",
      "Epoch 057: | Loss: 0.499 | Acc: 78.0\n",
      "Epoch 058: | Loss: 0.497 | Acc: 78.0\n",
      "Epoch 059: | Loss: 0.495 | Acc: 78.0\n",
      "Epoch 060: | Loss: 0.493 | Acc: 78.0\n",
      "Epoch 061: | Loss: 0.491 | Acc: 78.0\n",
      "Epoch 062: | Loss: 0.490 | Acc: 78.0\n",
      "Epoch 063: | Loss: 0.488 | Acc: 79.0\n",
      "Epoch 064: | Loss: 0.486 | Acc: 79.0\n",
      "Epoch 065: | Loss: 0.484 | Acc: 79.0\n",
      "Epoch 066: | Loss: 0.483 | Acc: 79.0\n",
      "Epoch 067: | Loss: 0.481 | Acc: 79.0\n",
      "Epoch 068: | Loss: 0.479 | Acc: 79.0\n",
      "Epoch 069: | Loss: 0.477 | Acc: 79.0\n",
      "Epoch 070: | Loss: 0.476 | Acc: 79.0\n",
      "Epoch 071: | Loss: 0.474 | Acc: 79.0\n",
      "Epoch 072: | Loss: 0.473 | Acc: 79.0\n",
      "Epoch 073: | Loss: 0.471 | Acc: 80.0\n",
      "Epoch 074: | Loss: 0.469 | Acc: 80.0\n",
      "Epoch 075: | Loss: 0.468 | Acc: 81.0\n",
      "Epoch 076: | Loss: 0.466 | Acc: 81.0\n",
      "Epoch 077: | Loss: 0.465 | Acc: 81.0\n",
      "Epoch 078: | Loss: 0.463 | Acc: 81.0\n",
      "Epoch 079: | Loss: 0.462 | Acc: 81.0\n",
      "Epoch 080: | Loss: 0.460 | Acc: 81.0\n",
      "Epoch 081: | Loss: 0.459 | Acc: 81.0\n",
      "Epoch 082: | Loss: 0.457 | Acc: 82.0\n",
      "Epoch 083: | Loss: 0.456 | Acc: 82.0\n",
      "Epoch 084: | Loss: 0.454 | Acc: 82.0\n",
      "Epoch 085: | Loss: 0.453 | Acc: 82.0\n",
      "Epoch 086: | Loss: 0.452 | Acc: 82.0\n",
      "Epoch 087: | Loss: 0.450 | Acc: 82.0\n",
      "Epoch 088: | Loss: 0.449 | Acc: 82.0\n",
      "Epoch 089: | Loss: 0.448 | Acc: 82.0\n",
      "Epoch 090: | Loss: 0.446 | Acc: 82.0\n",
      "Epoch 091: | Loss: 0.445 | Acc: 82.0\n",
      "Epoch 092: | Loss: 0.443 | Acc: 82.0\n",
      "Epoch 093: | Loss: 0.442 | Acc: 83.0\n",
      "Epoch 094: | Loss: 0.441 | Acc: 83.0\n",
      "Epoch 095: | Loss: 0.440 | Acc: 83.0\n",
      "Epoch 096: | Loss: 0.438 | Acc: 83.0\n",
      "Epoch 097: | Loss: 0.437 | Acc: 83.0\n",
      "Epoch 098: | Loss: 0.436 | Acc: 84.0\n",
      "Epoch 099: | Loss: 0.435 | Acc: 84.0\n",
      "Epoch 100: | Loss: 0.433 | Acc: 84.0\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(X_train).squeeze(-1)\n",
    "    loss = criterion(torch.sigmoid(y_pred), y_train)\n",
    "    acc = binary_acc(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1:03}: | Loss: {loss.item():.3f} | Acc: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
